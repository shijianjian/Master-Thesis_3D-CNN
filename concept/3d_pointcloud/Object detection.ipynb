{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# piont cloud separator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from pyntcloud import PyntCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_ranges(point_cloud):\n",
    "\n",
    "    x_max = x_min = point_cloud[0][0]\n",
    "    y_max = y_min = point_cloud[0][1]\n",
    "    z_max = z_min = point_cloud[0][2]\n",
    "    \n",
    "    for ind, coor in enumerate(point_cloud):\n",
    "        if coor[0] > x_max:\n",
    "            x_max = coor[0]\n",
    "        if coor[0] < x_min:\n",
    "            x_min = coor[0]\n",
    "        if coor[1] > y_max:\n",
    "            y_max = coor[1]\n",
    "        if coor[1] < y_min:\n",
    "            y_min = coor[1]\n",
    "        if coor[2] > z_max:\n",
    "            z_max = coor[2]\n",
    "        if coor[2] < z_min:\n",
    "            z_min = coor[2]\n",
    "            \n",
    "    return ((x_min, x_max), (y_min, y_max), (z_min, z_max))\n",
    "\n",
    "\n",
    "def segment_points(point_cloud, target_ranges):\n",
    "    \"\"\"\n",
    "    Recives orig point cloud data and a tuple of boundary tuples like:\n",
    "        ((x_min, x_max), (y_min, y_max), (z_min, z_max))\n",
    "    \"\"\"\n",
    "    \n",
    "    target_x_range, target_y_range, target_z_range = target_ranges[0], target_ranges[1], target_ranges[2]\n",
    "    \n",
    "    points = []\n",
    "    for ind, coor in enumerate(point_cloud):\n",
    "\n",
    "        if coor[0] >= target_x_range[0] and coor[0] <= target_x_range[1]:\n",
    "            if coor[1] >= target_y_range[0] and coor[1] <= target_y_range[1]:\n",
    "                if coor[2] >= target_z_range[0] and coor[2] <= target_z_range[1]:\n",
    "                    points.append(coor)\n",
    "                    \n",
    "    return np.asarray(points)\n",
    "\n",
    "\n",
    "def norm_point(point_cloud):\n",
    "    \n",
    "    ((x_min, x_max), (y_min, y_max), (z_min, z_max)) = find_ranges(point_cloud)\n",
    "    \n",
    "    biggest_value = np.max(np.asarray([x_max - x_min, y_max - y_min, z_max - z_min])) + 0.000000001\n",
    "    \n",
    "    normalized_points = []\n",
    "    for ind, coor in enumerate(point_cloud):\n",
    "        row = np.empty(len(coor))\n",
    "        row[0] = (coor[0] - x_min)/biggest_value\n",
    "        row[1] = (coor[1] - y_min)/biggest_value\n",
    "        row[2] = (coor[2] - z_min)/biggest_value\n",
    "        normalized_points.append(row)\n",
    "        \n",
    "    return np.asarray(normalized_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traverse_point(point_cloud, cube_size=(3,3,3), overall_size=[32,32,32], stride=(1,1,1), filter_points_num=None):\n",
    "    \"\"\"\n",
    "    It will return the target segment of point cloud.\n",
    "    \"\"\"\n",
    "    threshold = 0\n",
    "    \n",
    "    if filter_points_num is not None:\n",
    "        threshold = filter_points_num\n",
    "    \n",
    "    norm_point_cloud = norm_point(point_cloud)\n",
    "    xyz_length = 1/np.asarray(overall_size)\n",
    "\n",
    "    segmentations = []\n",
    "    areas = []\n",
    "    \n",
    "    x_num = int((overall_size[0] - cube_size[0]) / stride[0] + 1)\n",
    "    y_num = int((overall_size[1] - cube_size[1]) / stride[1] + 1)\n",
    "    z_num = int((overall_size[2] - cube_size[2]) / stride[2] + 1)\n",
    "    \n",
    "    covers_all = (x_num == (overall_size[0] - cube_size[0]) / stride[0] + 1) and (y_num == (overall_size[1] - cube_size[1]) / stride[1] + 1) and (z_num == (overall_size[2] - cube_size[2]) / stride[2] + 1)\n",
    "\n",
    "    for _x in range(x_num):\n",
    "        for _y in range(y_num):\n",
    "            for _z in range(z_num):\n",
    "                target_area = ((_x*stride[0], _x*stride[0] + cube_size[0]), (_y*stride[1], _y*stride[1] + cube_size[1]), (_z*stride[2], _z*stride[2] + cube_size[2]))\n",
    "    \n",
    "                target_x_range = (target_area[0][0]*xyz_length[0], target_area[0][1]*xyz_length[0])\n",
    "                target_y_range = (target_area[1][0]*xyz_length[1], target_area[1][1]*xyz_length[1])\n",
    "                target_z_range = (target_area[2][0]*xyz_length[2], target_area[2][1]*xyz_length[2])\n",
    "                \n",
    "                segmented_point_cloud = segment_points(norm_point_cloud, (target_x_range, target_y_range, target_z_range))\n",
    "\n",
    "                if len(segmented_point_cloud) > threshold:\n",
    "                    \n",
    "                    segmentations.append(segmented_point_cloud)\n",
    "                    \n",
    "                    areas.append(target_area)\n",
    "                    \n",
    "    # Add in the last piece of cube ()               \n",
    "    if not covers_all: \n",
    "        target_area = ((x_num*stride[0], x_num*stride[0] + cube_size[0]), (y_num*stride[1], y_num*stride[1] + cube_size[1]), (z_num*stride[2], z_num*stride[2] + cube_size[2]))\n",
    "    \n",
    "        target_x_range = (target_area[0][0]*xyz_length[0], target_area[0][1]*xyz_length[0])\n",
    "        target_y_range = (target_area[1][0]*xyz_length[1], target_area[1][1]*xyz_length[1])\n",
    "        target_z_range = (target_area[2][0]*xyz_length[2], target_area[2][1]*xyz_length[2])\n",
    "\n",
    "        segmented_point_cloud = segment_points(norm_point_cloud, (target_x_range, target_y_range, target_z_range))\n",
    "\n",
    "        if len(segmented_point_cloud) > threshold:\n",
    "            \n",
    "            segmentations.append(segmented_point_cloud)   \n",
    "            \n",
    "            areas.append(target_area)\n",
    "    \n",
    "    return {'data': segmentations, 'area': areas}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "my_point_cloud = PyntCloud.from_file(os.path.join(os.getcwd(), 'ttt2.pts'), sep=\" \", header=0, names=[\"x\",\"y\",\"z\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "segs = traverse_point(my_point_cloud.xyz, cube_size=(5,5,5), overall_size=[32,32,32], stride=(1,1,1), filter_points_num=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def voxelize3D(pts, dim=[1,1,1]):\n",
    "    \"\"\"\n",
    "    pts: receives .pts cloud point data. 2D array, arbitary sized X,Y,Z pairs. (We will only take x,y,z into account for now)\n",
    "    dim: dimensioin of output voxelized data\n",
    "    \n",
    "    This function will locate the grid cube and calculate the density of each cube.\n",
    "    The output will be normalized values.\n",
    "    \"\"\"\n",
    "    assert(pts.shape[1]>=3), \"pts file should contain at least x,y,z coordinate\"\n",
    "    assert(len(dim)==3), \"Please provide 3-d grid size like [32,32,32]\"\n",
    "    \n",
    "    if len(pts) > 1:\n",
    "        # move all the axis to positive area.\n",
    "        minimum_val = [pts[0][0], pts[0][1], pts[0][2]]\n",
    "\n",
    "        # find the smallest \n",
    "        for pair in pts:\n",
    "            if pair[0] < minimum_val[0]:\n",
    "                minimum_val[0] = pair[0]\n",
    "            if pair[1] < minimum_val[1]:\n",
    "                minimum_val[1] = pair[1]\n",
    "            if pair[2] < minimum_val[2]:\n",
    "                minimum_val[2] = pair[2]\n",
    "\n",
    "        # move it to first quadrant \n",
    "        rectified_pts = np.empty(pts.shape)\n",
    "        for index, pair in enumerate(pts):\n",
    "            point = np.zeros(3)\n",
    "            point[0] = pair[0] - minimum_val[0]\n",
    "            point[1] = pair[1] - minimum_val[1]\n",
    "            point[2] = pair[2] - minimum_val[2]\n",
    "            rectified_pts[index] = point\n",
    "\n",
    "        # biggest value in each axis \n",
    "        maximum_val = pts[0][0]\n",
    "\n",
    "        for pair in rectified_pts:\n",
    "            for val in pair:\n",
    "                if val > maximum_val:\n",
    "                    maximum_val = val\n",
    "\n",
    "        # normalize all the axises to (0,1)\n",
    "        normalized_pts = rectified_pts/maximum_val\n",
    "    \n",
    "    else:\n",
    "        # in case there is just one point\n",
    "        normalized_pts = pts\n",
    "    \n",
    "    x_grid_length = 1/dim[0]\n",
    "    y_grid_length = 1/dim[1]\n",
    "    z_grid_length = 1/dim[2]\n",
    "    \n",
    "    output = np.zeros((dim[0],dim[1],dim[2]))\n",
    "    \n",
    "    epsilon = 0.000000000001 # we will have at least a 1.0 value which will exceed the index of grid\n",
    "    # we can use a relativly small value to escape that to fit our data\n",
    "    \n",
    "    max_volume_size = 0\n",
    "    \n",
    "    for pair in normalized_pts:\n",
    "        x_loc = int(pair[0]/(x_grid_length + epsilon))\n",
    "        y_loc = int(pair[1]/(y_grid_length + epsilon))\n",
    "        z_loc = int(pair[2]/(z_grid_length + epsilon))\n",
    "        if output[x_loc, y_loc, z_loc] is None:\n",
    "            output[x_loc, y_loc, z_loc] = 1\n",
    "        else:\n",
    "            output[x_loc, y_loc, z_loc] += 1\n",
    "        \n",
    "        if output[x_loc, y_loc, z_loc] > max_volume_size:\n",
    "            max_volume_size = output[x_loc, y_loc, z_loc]\n",
    "    \n",
    "    output = output/max_volume_size    \n",
    "            \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13367\n",
      "1331\n",
      "1827\n"
     ]
    }
   ],
   "source": [
    "print(len(my_point_cloud.xyz))\n",
    "print(len(segs['data'][0]))\n",
    "print(len(segs['area']))\n",
    "vox_segs = []\n",
    "whole_vox = voxelize3D(my_point_cloud.xyz, dim=[32,32,32])\n",
    "whole_vox = whole_vox.reshape(whole_vox.shape + (1,))\n",
    "\n",
    "for idx, value in enumerate(segs['data']):\n",
    "    vox = voxelize3D(value, dim=[32,32,32])\n",
    "    vox_chan = np.array(vox).reshape(vox.shape + (1,))\n",
    "    vox_segs.append(vox_chan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "# Create hdf5\n",
    "hdf5_path = os.path.join(os.getcwd(), 'h5dataset', 'big_shuffled_sets.h5')\n",
    "hdf5_file = h5py.File(hdf5_path, mode='r')\n",
    "\n",
    "b_data = hdf5_file.get('voxels')\n",
    "b_labels = hdf5_file.get('labels')\n",
    "b_label_ref = hdf5_file.get('label_ref')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# one hot indexes\n",
    "\n",
    "def predict(voxels, output_format='weights', device_name='cpu:0'):\n",
    "    \"\"\"\n",
    "    Output_format can be 'weights' or 'probs'.\n",
    "    \n",
    "    'weights' will output the weight straight away which can be treated as confident.\n",
    "    \n",
    "    'probs' will output the corresponding probabilities for each class.\n",
    "    \"\"\"\n",
    "    \n",
    "    import os\n",
    "    model_path = os.path.join(os.getcwd(), 'trained_model', 'model-2')\n",
    "\n",
    "    config = tf.ConfigProto(allow_soft_placement = True)\n",
    "    with tf.Session(graph=tf.Graph(), config=config) as sess:\n",
    "        with tf.device(device_name):\n",
    "\n",
    "            saver = tf.train.import_meta_graph(model_path + \".meta\")\n",
    "            saver.restore(sess, model_path)\n",
    "\n",
    "            graph = tf.get_default_graph()\n",
    "            x_input = graph.get_tensor_by_name('inputs/x_input:0')\n",
    "            y_input = graph.get_tensor_by_name('inputs/y_input:0')\n",
    "            pred = graph.get_collection('logits')\n",
    "            accuracy = graph.get_tensor_by_name('acc:0')\n",
    "            \n",
    "            res = []\n",
    "            \n",
    "            if output_format is 'weights':\n",
    "                for _, val in enumerate(voxels):\n",
    "                    y = sess.run(pred, feed_dict={x_input: [val]})\n",
    "                    res.append(np.asarray(y[0]))\n",
    "                return res\n",
    "            \n",
    "            elif output_format is 'probs':\n",
    "                for _, val in enumerate(voxels):\n",
    "                    y = sess.run(pred, feed_dict={x_input: [val]})\n",
    "                    res.append(tf.nn.softmax(np.asarray(y[0])).eval())\n",
    "                return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/shijian/git/3D-CNN/concept/3d_pointcloud/trained_model/model-2\n"
     ]
    }
   ],
   "source": [
    "predictions = predict(vox_segs, output_format='weights', device_name='cpu:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_prob(probs, cube_labels, threshold=0.7):\n",
    "    li = dict()\n",
    "    cubes = dict()\n",
    "    for _, val in enumerate(probs):\n",
    "        _max = np.argmax(val)\n",
    "        if val[0][_max] > threshold:\n",
    "            if _max not in li:\n",
    "                li.update({_max:1})\n",
    "                m_li = list()\n",
    "                m_li.append(cube_labels[_])\n",
    "                cubes.update({_max: m_li})\n",
    "            else:\n",
    "                li.update({_max: li[_max]+1})\n",
    "                m_li = cubes[_max]\n",
    "                m_li.append(cube_labels[_])\n",
    "                cubes.update({_max: m_li})\n",
    "    return li, cubes\n",
    "\n",
    "def count_confidence(confidence, cube_labels, threshold=3):\n",
    "    li = dict()\n",
    "    cubes = dict()\n",
    "    for _, val in enumerate(confidence):\n",
    "        _max = np.argmax(val)\n",
    "        if val[0][_max] > threshold:\n",
    "            if _max not in li:\n",
    "                li.update({_max:1})\n",
    "                m_li = list()\n",
    "                m_li.append(cube_labels[_])\n",
    "                cubes.update({_max: m_li})\n",
    "            else:\n",
    "                li.update({_max: li[_max]+1})\n",
    "                m_li = cubes[_max]\n",
    "                m_li.append(cube_labels[_])\n",
    "                cubes.update({_max: m_li})\n",
    "    return li, cubes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_density(occ_list, upper=[32,32,32]):\n",
    "    res = np.zeros((upper[0], upper[1], upper[2]))/10\n",
    "    for occ_row in occ_list:\n",
    "        for _x in range(occ_row[0][1] - occ_row[0][0]):\n",
    "            for _y in range(occ_row[1][1] - occ_row[1][0]):\n",
    "                for _z in range(occ_row[2][1] - occ_row[2][0]):\n",
    "                    res[occ_row[0][0]+_x][occ_row[1][0]+_y][occ_row[2][0]+_z] = 1\n",
    "    return res\n",
    "\n",
    "def to_union(occ_list):\n",
    "    res = [[occ_list[0][0][0], occ_list[0][0][1]], [occ_list[0][1][0], occ_list[0][1][1]], [occ_list[0][2][0], occ_list[0][2][1]]]\n",
    "    for occ_row in occ_list:\n",
    "        if res[0][0] > occ_row[0][0]:\n",
    "            res[0][0] = occ_row[0][0]\n",
    "        if res[0][1] < occ_row[0][1]:\n",
    "            res[0][1] = occ_row[0][1]\n",
    "            \n",
    "        if res[1][0] > occ_row[1][0]:\n",
    "            res[1][0] = occ_row[1][0]\n",
    "        if res[1][1] < occ_row[1][1]:\n",
    "            res[1][1] = occ_row[1][1]\n",
    "            \n",
    "        if res[2][0] > occ_row[2][0]:\n",
    "            res[2][0] = occ_row[2][0]\n",
    "        if res[2][1] < occ_row[2][1]:\n",
    "            res[2][1] = occ_row[2][1]\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 60, 2: 91, 4: 133}\n",
      "[[1, 13], [4, 15], [6, 17]]\n",
      "[b'car' b'lamp' b'table' b'airplane' b'chair']\n"
     ]
    }
   ],
   "source": [
    "argmax, occ = count_confidence(predictions, segs['area'], threshold=4)\n",
    "print(argmax)\n",
    "print(to_union(occ[4]))\n",
    "print(b_label_ref.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1423\n"
     ]
    }
   ],
   "source": [
    "target = occ[1]\n",
    "for idx, v in enumerate(segs['area']):\n",
    "    if v in target:\n",
    "        print(idx, len(segs['data'][idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file = open(os.path.join(os.getcwd(),\"occ5out32.temp\"), \"w\") \n",
    "# file.write(str(segs))\n",
    "# file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_file(xyz, filename='xyz'):\n",
    "    import os\n",
    "    file = open(os.path.join(os.getcwd(), filename + \".pts\"), \"w\") \n",
    "    \n",
    "    for point in xyz:\n",
    "        st = \"\"\n",
    "        for item in point:\n",
    "            st += str(item) + \" \"\n",
    "        file.write(st.strip() + \"\\n\")\n",
    "\n",
    "    file.close() \n",
    "    \n",
    "# write_to_file(segmented_point_cloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ./voxel_grid_plot.py\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from IPython.display import IFrame\n",
    "from matplotlib import pyplot as plt\n",
    "from pyntcloud import PyntCloud\n",
    "\n",
    "def plot_voxel_points(voxelgrid,\n",
    "                   point_cloud,\n",
    "                   output_name=None,\n",
    "                   cmap=\"Oranges\",\n",
    "                   axis=True,\n",
    "                   width=800,\n",
    "                   height=600):\n",
    "\n",
    "    # For Voxel Grid\n",
    "    scaled_shape = voxelgrid.shape\n",
    "\n",
    "    vector = voxelgrid\n",
    "    points = np.argwhere(vector)\n",
    "\n",
    "    s_m = plt.cm.ScalarMappable(cmap=cmap)\n",
    "    rgb = s_m.to_rgba(vector.reshape(-1)[vector.reshape(-1) > 0])\n",
    "\n",
    "    camera_position = points.max(0) + abs(points.max(0))\n",
    "\n",
    "    look = points.mean(0)\n",
    "\n",
    "    if axis:\n",
    "        axis_size = points.ptp() * 1.5\n",
    "    else:\n",
    "        axis_size = 0\n",
    "    \n",
    "    # For point cloud\n",
    "    filename = 'pyntcloud_plot'\n",
    "    \n",
    "    path = os.path.join(os.getcwd(), filename + \".pts\")\n",
    "    # Fit point cloud into Voxel Grid\n",
    "    (x_min, x_max), (y_min, y_max), (z_min, z_max) = find_ranges(point_cloud)\n",
    "    pts_camera_position = (point_cloud.max(0) + abs(point_cloud.max(0))).tolist()\n",
    "    new_point_cloud = np.empty(point_cloud.shape)\n",
    "    for idx, row in enumerate(point_cloud):\n",
    "        _r = np.empty(row.shape)\n",
    "        _r[0] = (row[0] - x_min)/(x_max - x_min)*scaled_shape[0]\n",
    "        _r[1] = (row[1] - y_min)/(y_max - y_min)*scaled_shape[1]\n",
    "        _r[2] = (row[2] - z_min)/(z_max - z_min)*scaled_shape[2]\n",
    "        new_point_cloud[idx] = _r\n",
    "    point_cloud = new_point_cloud\n",
    "    \n",
    "    # Orange by default\n",
    "    colors = np.repeat([[255, 125, 0]], point_cloud.shape[0], axis=0)\n",
    "    colors = colors.astype(np.uint8)\n",
    "    points_df = pd.DataFrame(point_cloud, columns=[\"x\", \"y\", \"z\"])\n",
    "    for n, i in enumerate([\"red\", \"green\", \"blue\"]):\n",
    "        points_df[i] = colors[:, n]\n",
    "    cloud = PyntCloud(points_df)\n",
    "    \n",
    "    ply_gen = cloud.to_file(\"{}.ply\".format(filename), also_save=[\"mesh\"])\n",
    "    \n",
    "#     look_at = cloud.xyz.mean(0).tolist()\n",
    "    \n",
    "\n",
    "    placeholders = {}\n",
    "\n",
    "    placeholders[\"POINTS_X_PLACEHOLDER\"] = points[:, 0].tolist()\n",
    "    placeholders[\"POINTS_Y_PLACEHOLDER\"] = points[:, 1].tolist()\n",
    "    placeholders[\"POINTS_Z_PLACEHOLDER\"] = points[:, 2].tolist()\n",
    "\n",
    "    placeholders[\"R_PLACEHOLDER\"] = rgb[:, 0].tolist()\n",
    "    placeholders[\"G_PLACEHOLDER\"] = rgb[:, 1].tolist()\n",
    "    placeholders[\"B_PLACEHOLDER\"] = rgb[:, 2].tolist()\n",
    "\n",
    "    placeholders[\"S_x_PLACEHOLDER\"] = 1\n",
    "    placeholders[\"S_y_PLACEHOLDER\"] = 1\n",
    "    placeholders[\"S_z_PLACEHOLDER\"] = 1\n",
    "\n",
    "    placeholders[\"CAMERA_X_PLACEHOLDER\"] = camera_position[0]\n",
    "    placeholders[\"CAMERA_Y_PLACEHOLDER\"] = camera_position[1]\n",
    "    placeholders[\"CAMERA_Z_PLACEHOLDER\"] = camera_position[2]\n",
    "\n",
    "    placeholders[\"LOOK_X_PLACEHOLDER\"] = look[0]\n",
    "    placeholders[\"LOOK_Y_PLACEHOLDER\"] = look[1]\n",
    "    placeholders[\"LOOK_Z_PLACEHOLDER\"] = look[2]\n",
    "\n",
    "    placeholders[\"AXIS_SIZE_PLACEHOLDER\"] = axis_size\n",
    "\n",
    "    placeholders[\"N_VOXELS_PLACEHOLDER\"] = sum(vector.reshape(-1) > 0)\n",
    "    \n",
    "    placeholders[\"FILENAME_PLACEHOLDER\"] = \"\\\"\" + filename + \"\\\"\"\n",
    "    placeholders[\"POINT_SIZE_PLACEHOLDER\"] = 0.3\n",
    "\n",
    "    \n",
    "    if output_name is None:\n",
    "        output_name = \"plotVG.html\"\n",
    "    \n",
    "    BASE_PATH = os.getcwd()\n",
    "    src = \"{}/{}\".format(BASE_PATH, \"obj_detection_plot.html\")\n",
    "    dst = \"{}/{}\".format(os.getcwd(), output_name)\n",
    "    point_size=0.001\n",
    "    \n",
    "    with open(src, \"r\") as inp, open(dst, \"w\") as out:\n",
    "        for line in inp:\n",
    "            for key, val in placeholders.items():\n",
    "                if key in line:\n",
    "                    line = line.replace(key, str(val))\n",
    "            out.write(line)\n",
    "\n",
    "    return IFrame(output_name, width=width, height=height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 780,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'car' b'lamp' b'table' b'airplane' b'chair']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800\"\n",
       "            height=\"600\"\n",
       "            src=\"plotVG.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1a433db630>"
      ]
     },
     "execution_count": 780,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(b_label_ref.value)\n",
    "x = np.zeros((32,32,32))\n",
    "x[0:10, 0:15, 5:10] = 1\n",
    "plot_voxel_points(x, my_point_cloud.xyz)\n",
    "# plot_voxel_points(to_density(occ[1], upper=[32,32,32]), my_point_cloud.xyz)\n",
    "# my_point_cloud.plot(point_size=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion, we need to train our CNN with partial data at least for a better performance, since it will always treat partial data as a full data, wanted to give some predictions even it hasn't seen that data before.\n",
    "\n",
    "Next, I will crop some corner, side point to train the neural net to identify is it a valid object or not.\n",
    "\n",
    "Meanwhile, below is the implementation for unsupervised region proposal to avoid more training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Region Proposal\n",
    "### DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/NLeSC/PattyAnalytics/blob/master/patty/segmentation/dbscan.py\n",
    "\n",
    "from sklearn.cluster import dbscan\n",
    "def dbscan_labels(pointcloud, epsilon, minpoints, rgb_weight=0,\n",
    "                  algorithm='ball_tree'):\n",
    "    '''\n",
    "    Find an array of point-labels of clusters found by the DBSCAN algorithm.\n",
    "    Parameters\n",
    "    ----------\n",
    "    pointcloud : pcl.PointCloud\n",
    "        Input pointcloud.\n",
    "    epsilon : float\n",
    "        Neighborhood radius for DBSCAN.\n",
    "    minpoints : integer\n",
    "        Minimum neighborhood density for DBSCAN.\n",
    "    rgb_weight : float, optional\n",
    "        If non-zero, cluster on color information as well as location;\n",
    "        specifies the relative weight of the RGB components to spatial\n",
    "        coordinates in distance computations.\n",
    "        (RGB values have wildly different scales than spatial coordinates.)\n",
    "    Returns\n",
    "    -------\n",
    "    labels : Sequence\n",
    "        A sequence of labels per point. Label -1 indicates a point does not\n",
    "        belong to any cluster, other labels indicate the cluster number a\n",
    "        point belongs to.\n",
    "    '''\n",
    "\n",
    "    if rgb_weight > 0:\n",
    "        X = pointcloud.to_array()\n",
    "        X[:, 3:] *= rgb_weight\n",
    "    else:\n",
    "        X = pointcloud\n",
    "\n",
    "    _, labels = dbscan(X, eps=epsilon, min_samples=minpoints,\n",
    "                       algorithm=algorithm)\n",
    "    return np.asarray(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_shift_labels(pointcloud, bandwidth=None, seeds=None, bin_seeding=False,\n",
    "               min_bin_freq=1, cluster_all=True, max_iter=300,\n",
    "               n_jobs=1):\n",
    "    '''\n",
    "    Find an array of point-labels of clusters found by the DBSCAN algorithm.\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array-like, shape=[n_samples, n_features]\n",
    "        Input data.\n",
    "    bandwidth : float, optional\n",
    "        Kernel bandwidth.\n",
    "        If bandwidth is not given, it is determined using a heuristic based on\n",
    "        the median of all pairwise distances. This will take quadratic time in\n",
    "        the number of samples. The sklearn.cluster.estimate_bandwidth function\n",
    "        can be used to do this more efficiently.\n",
    "    seeds : array-like, shape=[n_seeds, n_features] or None\n",
    "        Point used as initial kernel locations. If None and bin_seeding=False,\n",
    "        each data point is used as a seed. If None and bin_seeding=True,\n",
    "        see bin_seeding.\n",
    "    bin_seeding : boolean, default=False\n",
    "        If true, initial kernel locations are not locations of all\n",
    "        points, but rather the location of the discretized version of\n",
    "        points, where points are binned onto a grid whose coarseness\n",
    "        corresponds to the bandwidth. Setting this option to True will speed\n",
    "        up the algorithm because fewer seeds will be initialized.\n",
    "        Ignored if seeds argument is not None.\n",
    "    min_bin_freq : int, default=1\n",
    "       To speed up the algorithm, accept only those bins with at least\n",
    "       min_bin_freq points as seeds.\n",
    "    cluster_all : boolean, default True\n",
    "        If true, then all points are clustered, even those orphans that are\n",
    "        not within any kernel. Orphans are assigned to the nearest kernel.\n",
    "        If false, then orphans are given cluster label -1.\n",
    "    max_iter : int, default 300\n",
    "        Maximum number of iterations, per seed point before the clustering\n",
    "        operation terminates (for that seed point), if has not converged yet.\n",
    "    n_jobs : int\n",
    "        The number of jobs to use for the computation. This works by computing\n",
    "        each of the n_init runs in parallel.\n",
    "    Returns\n",
    "    -------\n",
    "    cluster_centers : array, shape=[n_clusters, n_features]\n",
    "        Coordinates of cluster centers.\n",
    "    labels : array, shape=[n_samples]\n",
    "        Cluster labels for each point.\n",
    "    '''\n",
    "\n",
    "    _, labels = mean_shift(pointcloud, bandwidth=bandwidth, seeds=seeds, bin_seeding=bin_seeding,\n",
    "                            min_bin_freq=min_bin_freq, cluster_all=cluster_all, max_iter=max_iter,\n",
    "                            n_jobs=n_jobs)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import SpectralClustering\n",
    "clu = SpectralClustering()\n",
    "spe = clu.fit(c)\n",
    "spe.labels_\n",
    "res = find_cluster_points(c, spe.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_cloud = norm_point(my_point_cloud.xyz)\n",
    "lab = mean_shift_labels(normalized_cloud, max_iter=3000, n_jobs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = find_cluster_points(normalized_cloud, lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster.k_means_ import k_means\n",
    "# need exact number of clusters\n",
    "la = k_means(normalized_cloud, 6, init='k-means++', precompute_distances='auto',\n",
    "            n_init=10, max_iter=300, verbose=False,\n",
    "            tol=1e-4, random_state=None, copy_x=True, n_jobs=1,\n",
    "            algorithm=\"auto\", return_n_iter=False)\n",
    "res = find_cluster_points(normalized_cloud, la[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_cluster_points(point_cloud, labels):\n",
    "    res = dict()\n",
    "    for idx, val in enumerate(labels):\n",
    "        if val not in res:\n",
    "            li = list()\n",
    "            li.append(point_cloud[idx])\n",
    "            res.update({val: li})\n",
    "        else:\n",
    "            li = res[val]\n",
    "            li.append(point_cloud[idx])\n",
    "            res.update( {val: li} )\n",
    "    \n",
    "    renew_dict = dict()\n",
    "    for idx, key in enumerate(res):\n",
    "        renew_dict.update({key: np.asarray(res[key])})\n",
    "    \n",
    "    return renew_dict\n",
    "\n",
    "\n",
    "def localize_region(point_cloud, target_cloud, grid_size=[32,32,32]):\n",
    "    \n",
    "    ((x_min, x_max), (y_min, y_max), (z_min, z_max)) = find_ranges(point_cloud)\n",
    "    ((x_target_min, x_target_max), (y_target_min, y_target_max), (z_target_min, z_target_max)) = find_ranges(target_cloud)\n",
    "    \n",
    "    x_length = (x_max - x_min)/32\n",
    "    y_length = (y_max - y_min)/32\n",
    "    z_length = (z_max - z_min)/32\n",
    "    \n",
    "    x_region_min = int(x_target_min/x_length)\n",
    "    x_region_max = int(x_target_max/x_length + 2)\n",
    "    \n",
    "    y_region_min = int(y_target_min/y_length)\n",
    "    y_region_max = int(y_target_max/y_length + 2)\n",
    "    \n",
    "    z_region_min = int(z_target_min/z_length)\n",
    "    z_region_max = int(z_target_max/z_length + 2)\n",
    "    \n",
    "    return ((x_region_min, x_region_max), (y_region_min, y_region_max), (z_region_min, z_region_max))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_cloud = norm_point(my_point_cloud.xyz)\n",
    "cluster_labels = dbscan_labels(normalized_cloud, 0.02, 10, algorithm='ball_tree')\n",
    "res = find_cluster_points(normalized_cloud, cluster_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800\"\n",
       "            height=\"500\"\n",
       "            src=\"pyntcloud_plot.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x102eb6c88>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points_df = pd.DataFrame(res[3], columns=[\"x\", \"y\", \"z\"])\n",
    "PyntCloud(points_df).plot(point_size=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 4, 4, 4])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.16703001, -0.25178   ,  0.39102   ],\n",
       "       [-0.08656   , -0.30826   ,  0.2685    ],\n",
       "       [-0.07608   , -0.29803   ,  0.28215   ],\n",
       "       ..., \n",
       "       [ 0.00245   , -0.25911   ,  0.30232   ],\n",
       "       [-0.09092   , -0.30221   ,  0.2471    ],\n",
       "       [-0.12065   , -0.27172   ,  0.26181999]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_point_cloud = PyntCloud.from_file(os.path.join(os.getcwd(), 'ttt3.pts'), sep=\" \", header=0, names=[\"x\",\"y\",\"z\"])\n",
    "region = localize_region(normalized_cloud, res[0])\n",
    "den = np.zeros((32,32,32))\n",
    "den[region[0][0]:region[0][1], region[1][0]:region[1][1], region[2][0]:region[2][1]] = 1\n",
    "plot_voxel_points(den, my_point_cloud.xyz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 746,
   "metadata": {},
   "outputs": [],
   "source": [
    "voxels = []\n",
    "for idx, key in enumerate(res):\n",
    "    if key != -1:\n",
    "        vox = voxelize3D(res[key], dim=[32,32,32])\n",
    "        vox_chan = np.array(vox).reshape(vox.shape + (1,))\n",
    "        voxels.append(vox_chan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 753,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/shijian/git/3D-CNN/3d_pointcloud/trained_model/model-2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/shijian/git/3D-CNN/3d_pointcloud/trained_model/model-2\n"
     ]
    }
   ],
   "source": [
    "predictions = predict(voxels, output_format='probs', device_name='cpu:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 755,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'car' b'lamp' b'table' b'airplane' b'chair']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([[ 0.00343599,  0.04454208,  0.00514553,  0.89686203,  0.05001442]], dtype=float32),\n",
       " array([[  3.52182433e-05,   8.41522455e-01,   9.86838937e-02,\n",
       "           1.22957979e-04,   5.96354268e-02]], dtype=float32),\n",
       " array([[ 0.02688141,  0.7284373 ,  0.13400495,  0.00968067,  0.10099573]], dtype=float32),\n",
       " array([[  1.96535271e-11,   2.92222392e-11,   9.63168191e-07,\n",
       "           6.79611640e-13,   9.99999046e-01]], dtype=float32),\n",
       " array([[  1.08488756e-07,   9.49013156e-06,   9.46136415e-01,\n",
       "           7.13847754e-08,   5.38539365e-02]], dtype=float32)]"
      ]
     },
     "execution_count": 755,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print(b_label_ref.value)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# produce multi data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(13368, 3)\n"
     ]
    }
   ],
   "source": [
    "my_point_cloud_a = PyntCloud.from_file(os.path.join(os.getcwd(), 'PartAnnotation', 'airplane', 'points', '1a74b169a76e651ebc0909d98a1ff2b4.pts'), sep=\" \", header=0, names=[\"x\",\"y\",\"z\"])\n",
    "my_point_cloud_b = PyntCloud.from_file(os.path.join(os.getcwd(), 'PartAnnotation', 'lamp', 'points', '1a9c1cbf1ca9ca24274623f5a5d0bcdc.pts'), sep=\" \", header=0, names=[\"x\",\"y\",\"z\"])\n",
    "my_point_cloud_c = PyntCloud.from_file(os.path.join(os.getcwd(), 'PartAnnotation', 'car', 'points', '1a0c91c02ef35fbe68f60a737d94994a.pts'), sep=\" \", header=0, names=[\"x\",\"y\",\"z\"])\n",
    "my_point_cloud_d = PyntCloud.from_file(os.path.join(os.getcwd(), 'PartAnnotation', 'chair', 'points', '1ab8a3b55c14a7b27eaeab1f0c9120b7.pts'), sep=\" \", header=0, names=[\"x\",\"y\",\"z\"])\n",
    "my_point_cloud_e = PyntCloud.from_file(os.path.join(os.getcwd(), 'PartAnnotation', 'table', 'points', '1a1fb603583ce36fc3bd24f986301745.pts'), sep=\" \", header=0, names=[\"x\",\"y\",\"z\"])\n",
    "\n",
    "print(type(my_point_cloud_a.xyz))\n",
    "c = np.concatenate((my_point_cloud_a.xyz, my_point_cloud_b.xyz + 0.5, my_point_cloud_c.xyz + 1, my_point_cloud_d.xyz - 0.5, my_point_cloud_e.xyz - 1))\n",
    "print(c.shape)\n",
    "\n",
    "write_to_file(c, filename=\"ttt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800\"\n",
       "            height=\"500\"\n",
       "            src=\"pyntcloud_plot.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1a2a1576d8>"
      ]
     },
     "execution_count": 412,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PyntCloud.from_file(os.path.join(os.getcwd(), 'ttt2.pts'), sep=\" \", header=0, names=[\"x\",\"y\",\"z\"]).plot(point_size=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "my_point_cloud_b = PyntCloud.from_file(os.path.join(os.getcwd(), 'PartAnnotation', 'lamp', 'points', '1a9c1cbf1ca9ca24274623f5a5d0bcdc.pts'), sep=\" \", header=0, names=[\"x\",\"y\",\"z\"])\n",
    "my_point_cloud_e = PyntCloud.from_file(os.path.join(os.getcwd(), 'PartAnnotation', 'table', 'points', '1a1fb603583ce36fc3bd24f986301745.pts'), sep=\" \", header=0, names=[\"x\",\"y\",\"z\"])\n",
    "\n",
    "c = np.concatenate((my_point_cloud_b.xyz*0.35, my_point_cloud_e.xyz + np.asarray([0,-0.3,0])))\n",
    "\n",
    "points_df = pd.DataFrame(c, columns=[\"x\", \"y\", \"z\"])\n",
    "PyntCloud(points_df).plot()\n",
    "# write_to_file(c, filename=\"table_lamp_sample\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_ranges(point_cloud):\n",
    "\n",
    "    x_max = x_min = point_cloud[0][0]\n",
    "    y_max = y_min = point_cloud[0][1]\n",
    "    z_max = z_min = point_cloud[0][2]\n",
    "    \n",
    "    for ind, coor in enumerate(point_cloud):\n",
    "        if coor[0] > x_max:\n",
    "            x_max = coor[0]\n",
    "        if coor[0] < x_min:\n",
    "            x_min = coor[0]\n",
    "        if coor[1] > y_max:\n",
    "            y_max = coor[1]\n",
    "        if coor[1] < y_min:\n",
    "            y_min = coor[1]\n",
    "        if coor[2] > z_max:\n",
    "            z_max = coor[2]\n",
    "        if coor[2] < z_min:\n",
    "            z_min = coor[2]\n",
    "            \n",
    "    return ((x_min, x_max), (y_min, y_max), (z_min, z_max))\n",
    "\n",
    "\n",
    "def find_cube_length(t_range, overall_size):\n",
    "    length = (t_range[1] - t_range[0])/overall_size\n",
    "    return length\n",
    "\n",
    "def get_all_length(point_cloud, overall_size, xyz_ranges):\n",
    "    \n",
    "    x_length = find_cube_length(xyz_ranges[0], overall_size[0])\n",
    "    y_length = find_cube_length(xyz_ranges[1], overall_size[1])\n",
    "    z_length = find_cube_length(xyz_ranges[2], overall_size[2])\n",
    "    \n",
    "    return (x_length, y_length, z_length)\n",
    "    \n",
    "def find_target_area_range(point_cloud, target_area, xyz_length, xyz_ranges):\n",
    "    \n",
    "    target_x_range_max = xyz_ranges[0][1] - target_area[0][0]*xyz_length[0]\n",
    "    target_x_range_min = xyz_ranges[0][1] - target_area[0][1]*xyz_length[0]\n",
    "    target_y_range_max = xyz_ranges[1][1] - target_area[1][0]*xyz_length[1]\n",
    "    target_y_range_min = xyz_ranges[1][1] - target_area[1][1]*xyz_length[1]\n",
    "    target_z_range_max = xyz_ranges[2][1] - target_area[2][0]*xyz_length[2]\n",
    "    target_z_range_min = xyz_ranges[2][1] - target_area[2][1]*xyz_length[2]\n",
    "    \n",
    "    return (target_x_range_min, target_x_range_max), (target_y_range_min, target_y_range_max), (target_z_range_min, target_z_range_max)\n",
    "\n",
    "\n",
    "def segment_points(point_cloud, target_ranges):\n",
    "    \"\"\"\n",
    "    Recives orig point cloud data and a tuple of boundary tuples like:\n",
    "        ((x_min, x_max), (y_min, y_max), (z_min, z_max))\n",
    "    \"\"\"\n",
    "    \n",
    "    target_x_range, target_y_range, target_z_range = target_ranges[0], target_ranges[1], target_ranges[2]\n",
    "    \n",
    "    points = []\n",
    "    for ind, coor in enumerate(point_cloud):\n",
    "        if coor[0] >= target_x_range[0] and coor[0] <= target_x_range[1]:\n",
    "            if coor[1] >= target_y_range[0] and coor[1] <= target_y_range[1]:\n",
    "                if coor[2] >= target_z_range[0] and coor[2] <= target_z_range[1]:\n",
    "                    points.append(coor)\n",
    "                    \n",
    "    return np.asarray(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diagonal_cubes(point_cloud, overall_size=(16,16,16)):\n",
    "    \n",
    "    xyz_range = find_ranges(point_cloud)\n",
    "    xyz_length = get_all_length(point_cloud, overall_size, xyz_range)\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

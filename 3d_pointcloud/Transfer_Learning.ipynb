{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data comes from http://web.stanford.edu/~ericyi/project_page/part_annotation/index.html\n",
    "from pyntcloud import PyntCloud\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "import os\n",
    "if sys.platform == 'darwin':\n",
    "    data_path = os.getcwd() + \"/PartAnnotation\"\n",
    "else:\n",
    "    data_path = os.getcwd() + \"\\\\PartAnnotation\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_var_names():\n",
    "    all_vars = []\n",
    "    for i in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES):\n",
    "        all_vars.append(i.name)\n",
    "    return all_vars\n",
    "\n",
    "def get_all_placeholders():\n",
    "    return [x for x in tf.get_default_graph().get_operations() if x.type == \"Placeholder\"]\n",
    "\n",
    "def get_all_mean_op():\n",
    "    return [x for x in tf.get_default_graph().get_operations() if x.type == \"Mean\"]\n",
    "\n",
    "def remove_var(var_name, var_set):\n",
    "    if var_name in var_set:\n",
    "        var_set.remove(var_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:\\Users\\212606295\\Desktop\\3D-CNN\\3d_pointcloud\\trained_model\\model-2\n",
      "['conv1/kernel:0', 'conv1/bias:0', 'conv2/kernel:0', 'conv2/bias:0', 'conv4/kernel:0', 'conv4/bias:0', 'conv5/kernel:0', 'conv5/bias:0', 'conv7/kernel:0', 'conv7/bias:0', 'conv8/kernel:0', 'conv8/bias:0', 'bn/gamma:0', 'bn/beta:0', 'bn/moving_mean:0', 'bn/moving_variance:0', 'full_con/kernel:0', 'full_con/bias:0', 'y_pred/kernel:0', 'y_pred/bias:0', 'training/beta1_power:0', 'training/beta2_power:0', 'conv1/kernel/Adam:0', 'conv1/kernel/Adam_1:0', 'conv1/bias/Adam:0', 'conv1/bias/Adam_1:0', 'conv2/kernel/Adam:0', 'conv2/kernel/Adam_1:0', 'conv2/bias/Adam:0', 'conv2/bias/Adam_1:0', 'conv4/kernel/Adam:0', 'conv4/kernel/Adam_1:0', 'conv4/bias/Adam:0', 'conv4/bias/Adam_1:0', 'conv5/kernel/Adam:0', 'conv5/kernel/Adam_1:0', 'conv5/bias/Adam:0', 'conv5/bias/Adam_1:0', 'conv7/kernel/Adam:0', 'conv7/kernel/Adam_1:0', 'conv7/bias/Adam:0', 'conv7/bias/Adam_1:0', 'conv8/kernel/Adam:0', 'conv8/kernel/Adam_1:0', 'conv8/bias/Adam:0', 'conv8/bias/Adam_1:0', 'bn/gamma/Adam:0', 'bn/gamma/Adam_1:0', 'bn/beta/Adam:0', 'bn/beta/Adam_1:0', 'full_con/kernel/Adam:0', 'full_con/kernel/Adam_1:0', 'full_con/bias/Adam:0', 'full_con/bias/Adam_1:0', 'y_pred/kernel/Adam:0', 'y_pred/kernel/Adam_1:0', 'y_pred/bias/Adam:0', 'y_pred/bias/Adam_1:0']\n",
      "\n",
      "Optimizer [<tf.Operation 'training/Adam' type=NoOp>]\n",
      "\n",
      "Placeholders [<tf.Operation 'inputs/x_input' type=Placeholder>, <tf.Operation 'inputs/y_input' type=Placeholder>]\n",
      "\n",
      "Mean [<tf.Operation 'batch_norm/bn/moments/mean' type=Mean>, <tf.Operation 'batch_norm/bn/moments/variance' type=Mean>, <tf.Operation 'cross_entropy/cross_entropy' type=Mean>, <tf.Operation 'acc' type=Mean>]\n",
      "{'full_con/kernel', 'conv5/bias', 'conv1/bias', 'full_con/kernel/Adam_1', 'y_pred/kernel/Adam_1', 'conv1/kernel/Adam', 'conv4/bias/Adam', 'y_pred/bias', 'conv4/bias/Adam_1', 'conv8/bias', 'conv1/bias/Adam_1', 'full_con/bias/Adam_1', 'full_con/bias/Adam', 'full_con/bias', 'conv7/bias/Adam', 'conv5/kernel', 'bn/moving_variance', 'conv8/kernel/Adam', 'conv8/bias/Adam_1', 'y_pred/bias/Adam', 'conv4/bias', 'conv8/kernel/Adam_1', 'training/beta1_power', 'bn/moving_mean', 'y_pred/bias/Adam_1', 'conv4/kernel/Adam', 'training/beta2_power', 'conv1/kernel/Adam_1', 'conv2/bias/Adam_1', 'bn/beta/Adam', 'conv8/kernel', 'conv2/kernel', 'conv2/kernel/Adam', 'bn/gamma/Adam_1', 'y_pred/kernel/Adam', 'conv7/kernel', 'conv4/kernel/Adam_1', 'conv7/bias', 'conv7/kernel/Adam_1', 'conv1/bias/Adam', 'conv8/bias/Adam', 'conv7/bias/Adam_1', 'conv2/bias/Adam', 'full_con/kernel/Adam', 'conv5/kernel/Adam', 'conv1/kernel', 'y_pred/kernel', 'conv2/bias', 'conv5/bias/Adam_1', 'conv5/kernel/Adam_1', 'conv4/kernel', 'bn/beta', 'conv2/kernel/Adam_1', 'bn/gamma', 'conv5/bias/Adam', 'bn/beta/Adam_1', 'conv7/kernel/Adam', 'bn/gamma/Adam'}\n"
     ]
    }
   ],
   "source": [
    "new_graph = tf.Graph()\n",
    "config = tf.ConfigProto(allow_soft_placement=True) # allow passing gpu-trained model to a cpu machine\n",
    "with tf.Session(graph=new_graph, config=config) as sess:\n",
    "\n",
    "    import os\n",
    "    model_fqn = os.path.join(os.getcwd(), 'trained_model', \"model-2\")\n",
    "    saver = tf.train.import_meta_graph(model_fqn + \".meta\")\n",
    "    saver.restore(sess, model_fqn)\n",
    "\n",
    "    graph = tf.get_default_graph()\n",
    "    print(get_all_var_names())\n",
    "    print(\"\\nOptimizer\", graph.get_collection(\"optimizer\"))\n",
    "    print(\"\\nPlaceholders\", get_all_placeholders())\n",
    "    print(\"\\nMean\", get_all_mean_op())\n",
    "    previous_vars = {var.op.name for var in tf.global_variables()}\n",
    "#     previous_vars = [var_name for var_name, _ in tf.contrib.framework.list_variables('ckpt')]\n",
    "    print(previous_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def voxelize3D(pts, dim=[1,1,1]):\n",
    "    \"\"\"\n",
    "    pts: receives .pts cloud point data. 2D array, arbitary sized X,Y,Z pairs. (We will only take x,y,z into account for now)\n",
    "    dim: dimensioin of output voxelized data\n",
    "    \n",
    "    This function will locate the grid cube and calculate the density of each cube.\n",
    "    The output will be normalized values.\n",
    "    \"\"\"\n",
    "    assert(pts.shape[1]>=3), \"pts file should contain at least x,y,z coordinate\"\n",
    "    assert(len(dim)==3), \"Please provide 3-d grid size like [32,32,32]\"\n",
    "    \n",
    "    # move all the axis to positive area.\n",
    "    minimum_val = [pts[0][0], pts[0][1], pts[0][2]]\n",
    "\n",
    "    # find the smallest \n",
    "    for pair in pts:\n",
    "        if pair[0] < minimum_val[0]:\n",
    "            minimum_val[0] = pair[0]\n",
    "        if pair[1] < minimum_val[1]:\n",
    "            minimum_val[1] = pair[1]\n",
    "        if pair[2] < minimum_val[2]:\n",
    "            minimum_val[2] = pair[2]\n",
    "            \n",
    "    # move it to first quadrant \n",
    "    rectified_pts = np.empty(pts.shape)\n",
    "    for index, pair in enumerate(pts):\n",
    "        point = np.zeros(3)\n",
    "        point[0] = pair[0] - minimum_val[0]\n",
    "        point[1] = pair[1] - minimum_val[1]\n",
    "        point[2] = pair[2] - minimum_val[2]\n",
    "        rectified_pts[index] = point\n",
    "    \n",
    "    # biggest value in each axis \n",
    "    maximum_val = pts[0][0]\n",
    "    \n",
    "    for pair in rectified_pts:\n",
    "        for val in pair:\n",
    "            if val > maximum_val:\n",
    "                maximum_val = val\n",
    "     \n",
    "    # normalize all the axises to (0,1)\n",
    "    normalized_pts = rectified_pts/maximum_val\n",
    "    \n",
    "    x_grid_length = 1/dim[0]\n",
    "    y_grid_length = 1/dim[1]\n",
    "    z_grid_length = 1/dim[2]\n",
    "    \n",
    "    output = np.zeros((dim[0],dim[1],dim[2]))\n",
    "    \n",
    "    epsilon = 0.000000000001 # we will have at least a 1.0 value which will exceed the index of grid\n",
    "    # we can use a relativly small value to escape that to fit our data\n",
    "    \n",
    "    max_volume_size = 0\n",
    "    \n",
    "    for pair in normalized_pts:\n",
    "        x_loc = int(pair[0]/(x_grid_length + epsilon))\n",
    "        y_loc = int(pair[1]/(y_grid_length + epsilon))\n",
    "        z_loc = int(pair[2]/(z_grid_length + epsilon))\n",
    "        if output[x_loc, y_loc, z_loc] is None:\n",
    "            output[x_loc, y_loc, z_loc] = 1\n",
    "        else:\n",
    "            output[x_loc, y_loc, z_loc] += 1\n",
    "        \n",
    "        if output[x_loc, y_loc, z_loc] > max_volume_size:\n",
    "            max_volume_size = output[x_loc, y_loc, z_loc]\n",
    "    \n",
    "    output = output/max_volume_size    \n",
    "            \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(data_path, max_file_num=None, dim=[32,32,32]):\n",
    "    data = []\n",
    "    \n",
    "    target_dir_path = os.path.join(data_path, 'points')\n",
    "    path, dirs, files = os.walk(target_dir_path).__next__()\n",
    "    file_count = len(files)\n",
    "    \n",
    "    count = 0\n",
    "    for pts_data in os.scandir(target_dir_path):\n",
    "        if (max_file_num is None) or (count < max_file_num):\n",
    "            _path = os.path.join(data_path, 'points', pts_data.name)\n",
    "            pts = PyntCloud.from_file(_path, sep=\" \", header=0, names=[\"x\",\"y\",\"z\"])\n",
    "            _vox = voxelize3D(pts.xyz, dim=dim)\n",
    "            vox_chan = np.array(_vox).reshape(_vox.shape + (1,))\n",
    "            data.append(vox_chan)\n",
    "            count += 1\n",
    "        else:\n",
    "            break\n",
    "                \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_previous_vars(model_path):\n",
    "    \n",
    "    new_graph = tf.Graph()\n",
    "    config = tf.ConfigProto(allow_soft_placement=True) # allow passing gpu-trained model to a cpu machine\n",
    "    \n",
    "    with tf.Session(graph=new_graph, config=config) as sess:\n",
    "        \n",
    "        # Restore model\n",
    "        previous_vars = {var.op.name for var in tf.global_variables()}\n",
    "\n",
    "    return previous_vars\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3288, 32, 32, 32, 1) (3288, 12) [b'laptop' b'motorbike' b'pipes' b'knife' b'mug' b'pistol' b'guitar'\n",
      " b'skateboar' b'rocket' b'cap' b'earphone' b'bag']\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import os\n",
    "h5_data = h5py.File(os.path.join(os.getcwd(), 'small_with_pipe.h5'), mode='r')\n",
    "\n",
    "data = h5_data.get(\"voxels\")\n",
    "label = h5_data.get(\"labels\")\n",
    "label_ref = h5_data.get(\"label_ref\")\n",
    "\n",
    "print(data.shape, label.shape, label_ref.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "def cnn3d_model(x_train_data, label_size, stop_layer=0, keep_rate=0.7, seed=None):\n",
    "    \n",
    "    if seed is not None:\n",
    "        tf.set_random_seed(seed)\n",
    "        \n",
    "    stop_config = np.ones(14, dtype=bool)\n",
    "    for _ in range(stop_layer):\n",
    "        stop_config[_] = False\n",
    "    \n",
    "    with tf.name_scope(\"layer_a\"):\n",
    "        # conv => 32*32*32\n",
    "        conv1 = tf.layers.conv3d(inputs=x_train_data, filters=16, kernel_size=[3,3,3], padding='same',trainable=bool(stop_config[1]), activation=tf.nn.relu, name=\"conv1\", reuse=tf.AUTO_REUSE)\n",
    "        # conv => 32*32*32\n",
    "        conv2 = tf.layers.conv3d(inputs=conv1, filters=32, kernel_size=[3,3,3], padding='same',trainable=bool(stop_config[2]), activation=tf.nn.relu, name=\"conv2\", reuse=tf.AUTO_REUSE)\n",
    "        # pool => 16*16*16\n",
    "        pool3 = tf.layers.max_pooling3d(inputs=conv2, pool_size=[2, 2, 2], strides=2, name=\"pool3\")\n",
    "        \n",
    "    with tf.name_scope(\"layer_b\"):\n",
    "        # conv => 16*16*16\n",
    "        conv4 = tf.layers.conv3d(inputs=pool3, filters=64, kernel_size=[3,3,3], padding='same',trainable=bool(stop_config[4]), activation=tf.nn.relu, name=\"conv4\", reuse=tf.AUTO_REUSE)\n",
    "        # conv => 16*16*16\n",
    "        conv5 = tf.layers.conv3d(inputs=conv4, filters=128, kernel_size=[3,3,3], padding='same',trainable=bool(stop_config[5]), activation=tf.nn.relu, name=\"conv5\", reuse=tf.AUTO_REUSE)\n",
    "        # pool => 8*8*8\n",
    "        pool6 = tf.layers.max_pooling3d(inputs=conv5, pool_size=[2, 2, 2], strides=2, name=\"pool6\")\n",
    "        \n",
    "    with tf.name_scope(\"layer_c\"):\n",
    "        # conv => 8*8*8\n",
    "        conv7 = tf.layers.conv3d(inputs=pool6, filters=256, kernel_size=[3,3,3], padding='same',trainable=bool(stop_config[7]), activation=tf.nn.relu, name=\"conv7\", reuse=tf.AUTO_REUSE)\n",
    "        # conv => 8*8*8\n",
    "        conv8 = tf.layers.conv3d(inputs=conv7, filters=512, kernel_size=[3,3,3], padding='same',trainable=bool(stop_config[8]), activation=tf.nn.relu, name=\"conv8\", reuse=tf.AUTO_REUSE)\n",
    "        # pool => 4*4*4\n",
    "        pool9 = tf.layers.max_pooling3d(inputs=conv8, pool_size=[2, 2, 2], strides=2, name=\"pool9\")\n",
    "        \n",
    "    with tf.name_scope(\"batch_norm\"):\n",
    "        cnn3d_bn = tf.layers.batch_normalization(inputs=pool9, trainable=bool(stop_config[10]), name=\"bn\", reuse=tf.AUTO_REUSE)\n",
    "        \n",
    "    with tf.name_scope(\"fully_con\"):\n",
    "        flattening = tf.reshape(cnn3d_bn, [-1, 4*4*4*512])\n",
    "        dense = tf.layers.dense(inputs=flattening, units=1024, activation=tf.nn.relu, trainable=bool(stop_config[11]), name=\"full_con\", reuse=tf.AUTO_REUSE)\n",
    "        # (1-keep_rate) is the probability that the node will be kept\n",
    "        # training & trainable are different logic\n",
    "        dropout = tf.layers.dropout(inputs=dense, rate=keep_rate, training=bool(stop_config[12]), name=\"dropout\")\n",
    "        \n",
    "    with tf.name_scope(\"y_conv\"):\n",
    "        y_conv = tf.layers.dense(inputs=dropout, units=label_size, name=\"y_pred\", trainable=bool(stop_config[13]), reuse=tf.AUTO_REUSE)\n",
    "    \n",
    "    return y_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_streaming_metrics(prediction,label,num_classes):\n",
    "\n",
    "    with tf.name_scope(\"test\"):\n",
    "        # Convert (?, num_classes) into (num_classes) 1-D array\n",
    "        label_n = tf.argmax(label, 1)\n",
    "        prediction_n = tf.argmax(prediction, 1)\n",
    "        # Compute a per-batch confusion\n",
    "        batch_confusion = tf.confusion_matrix(label_n, prediction_n, num_classes=num_classes, name='batch_confusion')\n",
    "        # Create an accumulator variable to hold the counts\n",
    "        confusion = tf.Variable(tf.zeros([num_classes,num_classes], dtype=tf.int32), name='confusion')\n",
    "        # Create the update op for doing a \"+=\" accumulation on the batch\n",
    "        confusion_update = confusion.assign(confusion + batch_confusion )\n",
    "        # Cast counts to float so tf.summary.image renormalizes to [0,255]\n",
    "        confusion_image = tf.reshape(tf.cast(confusion, tf.float32), [1, num_classes, num_classes, 1])\n",
    "\n",
    "        tf.summary.image('confusion',confusion_image)\n",
    "        tf.summary.scalar('accuracy',accuracy)\n",
    "\n",
    "    return confusion_update,confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3288, 32, 32, 32, 1) (3288, 12)\n",
      "[<tf.Variable 'full_con/kernel:0' shape=(32768, 1024) dtype=float32_ref>, <tf.Variable 'full_con/bias:0' shape=(1024,) dtype=float32_ref>, <tf.Variable 'y_pred/kernel:0' shape=(1024, 12) dtype=float32_ref>, <tf.Variable 'y_pred/bias:0' shape=(12,) dtype=float32_ref>, <tf.Variable 'test/confusion:0' shape=(12, 12) dtype=int32_ref>]\n",
      "[<tf.Variable 'full_con/kernel:0' shape=(32768, 1024) dtype=float32_ref>, <tf.Variable 'full_con/bias:0' shape=(1024,) dtype=float32_ref>, <tf.Variable 'y_pred/kernel:0' shape=(1024, 12) dtype=float32_ref>, <tf.Variable 'y_pred/bias:0' shape=(12,) dtype=float32_ref>, <tf.Variable 'test/confusion:0' shape=(12, 12) dtype=int32_ref>]\n",
      "\tLost for 144 / 144 0.1917759\n",
      "\n",
      "\tLost for 144 / 144 0.2030155\n",
      "\n",
      "\tLost for 144 / 144 0.1317699\n",
      "\n",
      "\tLost for 144 / 144 0.0989507\n",
      "\n",
      "\tLost for 144 / 144 0.1741573\n",
      "\n",
      "\tLost for 144 / 144 0.1214164\n",
      "\n",
      "\tLost for 144 / 144 0.03398346\n",
      "\n",
      "\tLost for 144 / 144 0.05245372\n",
      "\n",
      "\tLost for 144 / 144 0.1072317\n",
      "\n",
      "\tLost for 144 / 144 0.05539748\n",
      "\n",
      "\tLost for 144 / 144 0.05023747\n",
      "\n",
      "\tLost for 144 / 144 0.03359114\n",
      "\n",
      "\tLost for 144 / 144 0.037428892\n",
      "\n",
      "\tLost for 144 / 144 0.05184742\n",
      "\n",
      "\tLost for 144 / 144 0.02440815\n",
      "\n",
      "\tLost for 144 / 144 0.09665987\n",
      "\n",
      "\tLost for 144 / 144 0.00735175\n",
      "\n",
      "\tLost for 144 / 144 0.015822948\n",
      "\n",
      "\tLost for 144 / 144 0.02310772\n",
      "\n",
      "\tLost for 144 / 144 0.00805503\n",
      "\n",
      "\tLost for 62 / 62\n",
      "\n",
      "Test Acc: 0.970766129032\n",
      "[[135   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0 104   0   0   0   2   0   1   1   0   0   0]\n",
      " [  0   0  88   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0 109   0   2   4   0   0   0   0   0]\n",
      " [  0   0   0   0  67   0   0   0   0   1   0   0]\n",
      " [  0   3   0   0   0  90   0   0   2   0   0   0]\n",
      " [  0   0   0   4   0   0 247   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0  38   0   0   0   0]\n",
      " [  0   2   0   0   0   0   0   3  23   0   0   0]\n",
      " [  0   0   0   0   1   0   0   0   0  14   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   1  13   1]\n",
      " [  0   0   0   0   1   0   0   0   0   0   0  30]]\n"
     ]
    }
   ],
   "source": [
    "# one hot indexes\n",
    "\n",
    "import os\n",
    "model_path = os.path.join(os.getcwd(), 'trained_model', 'model-2')\n",
    "\n",
    "previous_vars = get_previous_vars(model_path)\n",
    "\n",
    "learning_rate = 0.001\n",
    "keep_rate = 0.5\n",
    "batch_size = 16\n",
    "\n",
    "file_num =1000\n",
    "\n",
    "x_ = data[:]\n",
    "y_ = label[:]\n",
    "print(x_.shape, y_.shape)\n",
    "        \n",
    "split_point = int(0.7*len(x_))\n",
    "x_train = x_[:split_point]\n",
    "y_train = y_[:split_point]\n",
    "\n",
    "x_test = x_[split_point:]\n",
    "y_test = y_[split_point:]\n",
    "\n",
    "n_classes = len(y_train[0])\n",
    "\n",
    "device_name = '/gpu:1' \n",
    "\n",
    "# for stop_layer in range(14):\n",
    "stop_layer = 11\n",
    "with tf.Session(graph=tf.Graph(), config=tf.ConfigProto(allow_soft_placement=True)) as sess:\n",
    "    with tf.device(device_name):\n",
    "        with tf.name_scope('inputs'):\n",
    "            x_input = tf.placeholder(tf.float32, shape=[None, 32, 32, 32, 1], name=\"x_input\")\n",
    "            y_input = tf.placeholder(tf.float32, shape=[None, n_classes], name=\"y_input\") \n",
    "\n",
    "        prediction = cnn3d_model(x_input, n_classes, seed=1234, stop_layer=stop_layer, keep_rate=0.5)\n",
    "        tf.add_to_collection(\"logits\", prediction)\n",
    "\n",
    "        with tf.name_scope(\"cross_entropy\"):\n",
    "            cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=y_input), name=\"cross_entropy\")\n",
    "\n",
    "        with tf.name_scope(\"training\"):\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "            tf.add_to_collection(\"optimizer\", optimizer)\n",
    "\n",
    "        correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y_input, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"acc\")\n",
    "        confusion_update, confusion = _get_streaming_metrics(prediction,y_input,n_classes)\n",
    "\n",
    "    tf.summary.scalar(\"cross_entropy\", cost)\n",
    "    tf.summary.scalar(\"accuracy\", accuracy)\n",
    "    summary_all = tf.summary.merge_all()\n",
    "    logs_path = os.path.join(os.getcwd(), 'summaries', 'confusion_matrix')\n",
    "    #suffix\n",
    "    filename_suffix = \"_keep_rate_\"+str(keep_rate)+\"_batch_sizes_\"+str(batch_size)+\"_learning_rates_\"+str(learning_rate)+\"_stop_layer_\"+str(stop_layer)\n",
    "    train_writer = tf.summary.FileWriter(os.path.join(logs_path, 'train'), graph=tf.get_default_graph(), filename_suffix=filename_suffix)\n",
    "    test_writer = tf.summary.FileWriter(os.path.join(logs_path, 'test'), graph=tf.get_default_graph(), filename_suffix=filename_suffix)\n",
    "    summary_op = {'summary': summary_all, 'train_writer': train_writer, 'test_writer': test_writer}\n",
    "\n",
    "    print(tf.trainable_variables())\n",
    "    remove_var('y_pred/kernel',previous_vars)\n",
    "    remove_var('y_pred/bias',previous_vars)\n",
    "    remove_var('y_pred/kernel/Adam',previous_vars)\n",
    "    remove_var('y_pred/bias/Adam',previous_vars)\n",
    "    remove_var('y_pred/kernel/Adam_1',previous_vars)\n",
    "    remove_var('y_pred/bias/Adam_1',previous_vars)\n",
    "\n",
    "    restore_map = {variable.op.name : variable for variable in tf.global_variables() if variable.op.name in previous_vars}\n",
    "    tf.contrib.framework.init_from_checkpoint(os.path.join(os.getcwd(), 'trained_model', 'model-2'), restore_map)\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    print(tf.trainable_variables())\n",
    "\n",
    "    graph = tf.get_default_graph()\n",
    "\n",
    "\n",
    "    iterations_all = int(len(x_)/batch_size) +1\n",
    "    iterations_train = int(len(x_train)/batch_size) +1\n",
    "    iterations_test= int(len(x_test)/batch_size) +1\n",
    "\n",
    "    for epoch in range(20):\n",
    "        for itr in range(iterations_train):\n",
    "            mini_batch_x = x_train[itr*batch_size: (itr+1)*batch_size]\n",
    "            mini_batch_y = y_train[itr*batch_size: (itr+1)*batch_size]\n",
    "            _optimizer, _cost, _summary = sess.run([optimizer, cost, summary_op['summary']], feed_dict={x_input: mini_batch_x, y_input: mini_batch_y})\n",
    "            print('\\tLost for', itr+1, \"/\", iterations_train, _cost, end='\\r')\n",
    "\n",
    "        summary_op['train_writer'].add_summary(_summary, epoch)\n",
    "        summary_op['train_writer'].flush()\n",
    "        print('\\n')\n",
    "\n",
    "    acc = 0\n",
    "    for itr in range(iterations_test):\n",
    "        mini_batch_x = x_test[itr*batch_size: (itr+1)*batch_size]\n",
    "        mini_batch_y = y_test[itr*batch_size: (itr+1)*batch_size]\n",
    "        _acc, _summary, _conf = sess.run([accuracy, summary_op['summary'], confusion_update], feed_dict={x_input: mini_batch_x, y_input: mini_batch_y})\n",
    "        acc += _acc\n",
    "        print('\\tLost for', itr+1, \"/\", iterations_test, end='\\r')\n",
    "\n",
    "    summary_op['train_writer'].add_summary(_summary, epoch)\n",
    "    summary_op['train_writer'].flush()\n",
    "    print('\\n')\n",
    "    print('Test Acc:', acc/iterations_test)\n",
    "    print(confusion.eval())\n",
    "\n",
    "    summary_op['train_writer'].close()\n",
    "    summary_op['test_writer'].close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'laptop', b'motorbike', b'pipes', b'knife', b'mug', b'pistol',\n",
       "       b'guitar', b'skateboar', b'rocket', b'cap', b'earphone', b'bag'],\n",
       "      dtype='|S9')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_ref.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ./voxel_grid_plot.py\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from IPython.display import IFrame\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "def plot_voxelgrid(voxelgrid,\n",
    "                   output_name=None,\n",
    "                   cmap=\"Oranges\",\n",
    "                   axis=True,\n",
    "                   width=800,\n",
    "                   height=600):\n",
    "\n",
    "    scaled_shape = voxelgrid.shape\n",
    "\n",
    "    vector = voxelgrid\n",
    "    points = np.argwhere(vector) * scaled_shape\n",
    "\n",
    "    s_m = plt.cm.ScalarMappable(cmap=cmap)\n",
    "    rgb = s_m.to_rgba(vector.reshape(-1)[vector.reshape(-1) > 0])\n",
    "\n",
    "    camera_position = points.max(0) + abs(points.max(0))\n",
    "\n",
    "    look = points.mean(0)\n",
    "\n",
    "    if axis:\n",
    "        axis_size = points.ptp() * 1.5\n",
    "    else:\n",
    "        axis_size = 0\n",
    "\n",
    "    placeholders = {}\n",
    "\n",
    "    placeholders[\"POINTS_X_PLACEHOLDER\"] = points[:, 0].tolist()\n",
    "    placeholders[\"POINTS_Y_PLACEHOLDER\"] = points[:, 1].tolist()\n",
    "    placeholders[\"POINTS_Z_PLACEHOLDER\"] = points[:, 2].tolist()\n",
    "\n",
    "    placeholders[\"R_PLACEHOLDER\"] = rgb[:, 0].tolist()\n",
    "    placeholders[\"G_PLACEHOLDER\"] = rgb[:, 1].tolist()\n",
    "    placeholders[\"B_PLACEHOLDER\"] = rgb[:, 2].tolist()\n",
    "\n",
    "    placeholders[\"S_x_PLACEHOLDER\"] = scaled_shape[0]\n",
    "    placeholders[\"S_y_PLACEHOLDER\"] = scaled_shape[1]\n",
    "    placeholders[\"S_z_PLACEHOLDER\"] = scaled_shape[2]\n",
    "\n",
    "    placeholders[\"CAMERA_X_PLACEHOLDER\"] = camera_position[0]\n",
    "    placeholders[\"CAMERA_Y_PLACEHOLDER\"] = camera_position[1]\n",
    "    placeholders[\"CAMERA_Z_PLACEHOLDER\"] = camera_position[2]\n",
    "\n",
    "    placeholders[\"LOOK_X_PLACEHOLDER\"] = look[0]\n",
    "    placeholders[\"LOOK_Y_PLACEHOLDER\"] = look[1]\n",
    "    placeholders[\"LOOK_Z_PLACEHOLDER\"] = look[2]\n",
    "\n",
    "    placeholders[\"AXIS_SIZE_PLACEHOLDER\"] = axis_size\n",
    "\n",
    "    placeholders[\"N_VOXELS_PLACEHOLDER\"] = sum(vector.reshape(-1) > 0)\n",
    "\n",
    "    if output_name is None:\n",
    "        output_name = \"plotVG.html\"\n",
    "\n",
    "    BASE_PATH = os.getcwd()\n",
    "    src = \"{}/{}\".format(BASE_PATH, \"voxelgrid.html\")\n",
    "    dst = \"{}/{}\".format(os.getcwd(), output_name)\n",
    "\n",
    "    with open(src, \"r\") as inp, open(dst, \"w\") as out:\n",
    "        for line in inp:\n",
    "            for key, val in placeholders.items():\n",
    "                if key in line:\n",
    "                    line = line.replace(key, str(val))\n",
    "            out.write(line)\n",
    "\n",
    "    return IFrame(output_name, width=width, height=height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/shijian/git/3D-CNN/3d_pointcloud/trained_model/model-4\n",
      "(3, 3, 3, 1, 16)\n"
     ]
    }
   ],
   "source": [
    "# one hot indexes\n",
    "# 2: chair , 3: car\n",
    "\n",
    "import os\n",
    "model_path = os.path.join(os.getcwd(), 'trained_model', 'model-4')\n",
    "\n",
    "params = get_model_params(model_path)\n",
    "\n",
    "print(params['conv1_w'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_num =7000\n",
    "\n",
    "a = get_data(data_path + \"/airplane\", max_file_num=file_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_padding(target, kernel, stride, padding='SAME'):\n",
    "    # Algorithm comes from https://github.com/tensorflow/tensorflow/commit/a276e0999ab4223ac36d75221028d3e8835c60ae\n",
    "    size, kernelsize, color_channels = get_shapes(target, kernel)\n",
    "    \n",
    "    import math\n",
    "    if padding is 'SAME':\n",
    "        out_width = math.ceil(float(size) / float(stride))\n",
    "        pad_along_width = max((out_width - 1) * stride + kernelsize - size, 0)\n",
    "        pad_left_and_top = pad_along_width // 2\n",
    "        pad_right_and_bottom = pad_along_width - pad_left_and_top\n",
    "        padded_target = zero_padding_3d(target, (int((pad_left_and_top)), int(pad_right_and_bottom)))\n",
    "        \n",
    "    elif padding is 'VALID':\n",
    "        # no padding would be used\n",
    "        out_width = math.floor(float(size - kernelsize) / float(stride) + 1)\n",
    "        border = (out_width-1)*stride + kernelsize\n",
    "        print(out_width, border)\n",
    "        padded_target = target[:border,:border,:border, :]\n",
    "        \n",
    "    else:\n",
    "        raise TypeError('Padding strategy `' + padding + '` not found.')\n",
    "        \n",
    "    print(\"Padded shape\", padded_target.shape)\n",
    "    return padded_target\n",
    "\n",
    "\n",
    "def zero_padding_3d(target, padding=None):\n",
    "    if padding is not None:\n",
    "        target = np.pad(target, [(padding[0], padding[1]), (padding[0], padding[1]), (padding[0], padding[1]), (0,0)], mode='constant')\n",
    "    return target\n",
    "\n",
    "\n",
    "def single_convolve_3d(padded_target_slice, kernel):\n",
    "    _slice = padded_target_slice * kernel\n",
    "    res = np.sum(np.sum(np.sum(np.sum(_slice, axis=0), axis=0),axis=0), axis=0)\n",
    "    return res\n",
    "\n",
    "\n",
    "def convole_3d(padded_target, kernel, kernel_bias, stride):\n",
    "    \n",
    "    size, kernelsize, color_channels = get_shapes(padded_target, kernel)\n",
    "    outsize = get_outsize(size, kernelsize, stride)\n",
    "    filter_num = num_of_filters(kernel)\n",
    "    \n",
    "    res = np.zeros((outsize, outsize, outsize, filter_num))\n",
    "    \n",
    "    for i in range(outsize):\n",
    "        for j in range(outsize):\n",
    "            for k in range(outsize):\n",
    "                for f in range(filter_num):\n",
    "                    i_start = i * stride\n",
    "                    j_start = j * stride\n",
    "                    k_start = k * stride\n",
    "\n",
    "                    padded_target_slice = padded_target[i_start : i_start+kernelsize, j_start : j_start+kernelsize, k_start : k_start+kernelsize, :]\n",
    "                    res[i_start, j_start, k_start, f] = single_convolve_3d(padded_target_slice, kernel[:,:,:,:,f])\n",
    "                    \n",
    "    print(res.shape)\n",
    "    return res + kernel_bias\n",
    "   \n",
    "def max_pool_slice(target_slice):\n",
    "    return np.max(target_slice)\n",
    "\n",
    "def get_shapes(target, kernel):\n",
    "    size = target.shape[0]\n",
    "    color_channels = target.shape[3]\n",
    "    kernelsize = kernel.shape[0]\n",
    "    assert(color_channels == kernel.shape[3]), \"The shape of kernel and input does not match, it must have same number of channels.\"\n",
    "    return size, kernelsize, color_channels\n",
    "\n",
    "def get_outsize(size, kernelsize, stride):\n",
    "    outsize = (size-kernelsize)/stride + 1\n",
    "    assert(outsize == int(outsize)), \"Number of filters must be integer\"\n",
    "    \n",
    "    return int(outsize)\n",
    "\n",
    "def num_of_filters(kernel):\n",
    "    return kernel.shape[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_features(target, kernel, bias=None, stride=1, padding='SAME'):\n",
    "    \n",
    "    size, kernelsize, color_channels = get_shapes(target, kernel)\n",
    "    outsize = get_outsize(size, kernelsize, stride)\n",
    "    filter_num = num_of_filters(kernel)\n",
    "    \n",
    "    padded_target = do_padding(target, kernel, stride)\n",
    "    output = convole_3d(padded_target, kernel, bias, stride)\n",
    "    \n",
    "    print(output.shape)\n",
    "    return output\n",
    "\n",
    "\n",
    "def max_pooling(target, pool_size=2, stride=2):\n",
    "    \n",
    "    size = target.shape[0]\n",
    "    outsize = get_outsize(size, pool_size, stride)\n",
    "    filter_num = target.shape[3]\n",
    "    \n",
    "    output = np.zeros((outsize, outsize, outsize, filter_num))\n",
    "    for i in range(outsize):\n",
    "        for j in range(outsize):\n",
    "            for k in range(outsize):\n",
    "                for f in range(filter_num):\n",
    "                    i_start = i*stride\n",
    "                    j_start = j*stride\n",
    "                    k_start = k*stride\n",
    "                    \n",
    "                    output[i, j, k, f] = max_pool_slice(target[i_start:i_start+2, j_start:j_start+2, k_start:k_start+2, f])\n",
    "                    \n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded shape (34, 34, 34, 1)\n",
      "(32, 32, 32, 16)\n",
      "(32, 32, 32, 16)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800\"\n",
       "            height=\"600\"\n",
       "            src=\"plotVG.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1a190245f8>"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_conv = output_features(a[0], params['conv1_w'], params['conv1_b'], stride=1, padding='SAME')\n",
    "plot_voxelgrid(first_conv[:,:,:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded shape (34, 34, 34, 16)\n",
      "(32, 32, 32, 32)\n",
      "(32, 32, 32, 32)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800\"\n",
       "            height=\"600\"\n",
       "            src=\"plotVG.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1a19024748>"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_conv = output_features(first_conv, params['conv2_w'], params['conv2_b'], stride=1, padding='SAME')\n",
    "plot_voxelgrid(second_conv[:,:,:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800\"\n",
       "            height=\"600\"\n",
       "            src=\"plotVG.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1a17fb1668>"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "third_pool = max_pooling(second_conv, pool_size=2, stride=2)\n",
    "plot_voxelgrid(third_pool[:,:,:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded shape (18, 18, 18, 32)\n",
      "(16, 16, 16, 64)\n",
      "(16, 16, 16, 64)\n",
      "Padded shape (18, 18, 18, 64)\n",
      "(16, 16, 16, 128)\n",
      "(16, 16, 16, 128)\n"
     ]
    }
   ],
   "source": [
    "forth_conv = output_features(third_pool, params['conv4_w'], params['conv4_b'], stride=1, padding='SAME')\n",
    "fifth_conv = output_features(forth_conv, params['conv5_w'], params['conv5_b'], stride=1, padding='SAME')\n",
    "sixth_pooling = max_pooling(fifth_conv, pool_size=2, stride=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800\"\n",
       "            height=\"600\"\n",
       "            src=\"plotVG.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1a17f1d748>"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_voxelgrid(sixth_pooling[:,:,:,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded shape (10, 10, 10, 128)\n",
      "(8, 8, 8, 256)\n",
      "(8, 8, 8, 256)\n",
      "Padded shape (10, 10, 10, 256)\n",
      "(8, 8, 8, 512)\n",
      "(8, 8, 8, 512)\n"
     ]
    }
   ],
   "source": [
    "seventh_conv = output_features(sixth_pooling, params['conv7_w'], params['conv7_b'], stride=1, padding='SAME')\n",
    "eighth_conv = output_features(seventh_conv, params['conv8_w'], params['conv8_b'], stride=1, padding='SAME')\n",
    "ninth_pooling = max_pooling(eighth_conv, pool_size=2, stride=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_voxelgrid(sixth_pooling[:,:,:,10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

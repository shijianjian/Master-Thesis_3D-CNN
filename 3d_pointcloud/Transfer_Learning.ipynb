{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data comes from http://web.stanford.edu/~ericyi/project_page/part_annotation/index.html\n",
    "from pyntcloud import PyntCloud\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "import os\n",
    "if sys.platform == 'darwin':\n",
    "    data_path = os.getcwd() + \"/PartAnnotation\"\n",
    "else:\n",
    "    data_path = os.getcwd() + \"\\\\PartAnnotation\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_var_names():\n",
    "    all_vars = []\n",
    "    for i in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES):\n",
    "        all_vars.append(i.name)\n",
    "    return all_vars\n",
    "\n",
    "def get_all_placeholders():\n",
    "    return [x for x in tf.get_default_graph().get_operations() if x.type == \"Placeholder\"]\n",
    "\n",
    "def get_all_mean_op():\n",
    "    return [x for x in tf.get_default_graph().get_operations() if x.type == \"Mean\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:\\Users\\212606295\\Desktop\\3D-CNN\\3d_pointcloud\\trained_model\\model-0\n"
     ]
    }
   ],
   "source": [
    "new_graph = tf.Graph()\n",
    "config = tf.ConfigProto(allow_soft_placement=True) # allow passing gpu-trained model to a cpu machine\n",
    "with tf.Session(graph=new_graph, config=config) as sess:\n",
    "\n",
    "    import os\n",
    "    model_fqn = os.path.join(os.getcwd(), 'trained_model', \"model-0\")\n",
    "    saver = tf.train.import_meta_graph(model_fqn + \".meta\")\n",
    "    saver.restore(sess, model_fqn)\n",
    "\n",
    "    graph = tf.get_default_graph()\n",
    "    print(get_all_var_names())\n",
    "    print(\"\\nOptimizer\", graph.get_collection(\"optimizer\"))\n",
    "    print(\"\\nPlaceholders\", get_all_placeholders())\n",
    "    print(\"\\nMean\", get_all_mean_op())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def voxelize3D(pts, dim=[1,1,1]):\n",
    "    \"\"\"\n",
    "    pts: receives .pts cloud point data. 2D array, arbitary sized X,Y,Z pairs. (We will only take x,y,z into account for now)\n",
    "    dim: dimensioin of output voxelized data\n",
    "    \n",
    "    This function will locate the grid cube and calculate the density of each cube.\n",
    "    The output will be normalized values.\n",
    "    \"\"\"\n",
    "    assert(pts.shape[1]>=3), \"pts file should contain at least x,y,z coordinate\"\n",
    "    assert(len(dim)==3), \"Please provide 3-d grid size like [32,32,32]\"\n",
    "    \n",
    "    # move all the axis to positive area.\n",
    "    minimum_val = [pts[0][0], pts[0][1], pts[0][2]]\n",
    "\n",
    "    # find the smallest \n",
    "    for pair in pts:\n",
    "        if pair[0] < minimum_val[0]:\n",
    "            minimum_val[0] = pair[0]\n",
    "        if pair[1] < minimum_val[1]:\n",
    "            minimum_val[1] = pair[1]\n",
    "        if pair[2] < minimum_val[2]:\n",
    "            minimum_val[2] = pair[2]\n",
    "            \n",
    "    # move it to first quadrant \n",
    "    rectified_pts = np.empty(pts.shape)\n",
    "    for index, pair in enumerate(pts):\n",
    "        point = np.zeros(3)\n",
    "        point[0] = pair[0] - minimum_val[0]\n",
    "        point[1] = pair[1] - minimum_val[1]\n",
    "        point[2] = pair[2] - minimum_val[2]\n",
    "        rectified_pts[index] = point\n",
    "    \n",
    "    # biggest value in each axis \n",
    "    maximum_val = pts[0][0]\n",
    "    \n",
    "    for pair in rectified_pts:\n",
    "        for val in pair:\n",
    "            if val > maximum_val:\n",
    "                maximum_val = val\n",
    "     \n",
    "    # normalize all the axises to (0,1)\n",
    "    normalized_pts = rectified_pts/maximum_val\n",
    "    \n",
    "    x_grid_length = 1/dim[0]\n",
    "    y_grid_length = 1/dim[1]\n",
    "    z_grid_length = 1/dim[2]\n",
    "    \n",
    "    output = np.zeros((dim[0],dim[1],dim[2]))\n",
    "    \n",
    "    epsilon = 0.000000000001 # we will have at least a 1.0 value which will exceed the index of grid\n",
    "    # we can use a relativly small value to escape that to fit our data\n",
    "    \n",
    "    max_volume_size = 0\n",
    "    \n",
    "    for pair in normalized_pts:\n",
    "        x_loc = int(pair[0]/(x_grid_length + epsilon))\n",
    "        y_loc = int(pair[1]/(y_grid_length + epsilon))\n",
    "        z_loc = int(pair[2]/(z_grid_length + epsilon))\n",
    "        if output[x_loc, y_loc, z_loc] is None:\n",
    "            output[x_loc, y_loc, z_loc] = 1\n",
    "        else:\n",
    "            output[x_loc, y_loc, z_loc] += 1\n",
    "        \n",
    "        if output[x_loc, y_loc, z_loc] > max_volume_size:\n",
    "            max_volume_size = output[x_loc, y_loc, z_loc]\n",
    "    \n",
    "    output = output/max_volume_size    \n",
    "            \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(data_path, max_file_num=None, dim=[32,32,32]):\n",
    "    data = []\n",
    "    \n",
    "    target_dir_path = os.path.join(data_path, 'points')\n",
    "    path, dirs, files = os.walk(target_dir_path).__next__()\n",
    "    file_count = len(files)\n",
    "    \n",
    "    count = 0\n",
    "    for pts_data in os.scandir(target_dir_path):\n",
    "        if (max_file_num is None) or (count < max_file_num):\n",
    "            _path = os.path.join(data_path, 'points', pts_data.name)\n",
    "            pts = PyntCloud.from_file(_path, sep=\" \", header=0, names=[\"x\",\"y\",\"z\"])\n",
    "            _vox = voxelize3D(pts.xyz, dim=dim)\n",
    "            vox_chan = np.array(_vox).reshape(_vox.shape + (1,))\n",
    "            data.append(vox_chan)\n",
    "            count += 1\n",
    "        else:\n",
    "            break\n",
    "                \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_params(model_path):\n",
    "    \n",
    "    new_graph = tf.Graph()\n",
    "    config = tf.ConfigProto(allow_soft_placement=True) # allow passing gpu-trained model to a cpu machine\n",
    "    \n",
    "    with tf.Session(graph=new_graph, config=config) as sess:\n",
    "        \n",
    "        # Restore model\n",
    "        saver = tf.train.import_meta_graph(model_path + \".meta\")\n",
    "        saver.restore(sess, model_path)\n",
    "\n",
    "        model_graph = tf.get_default_graph()\n",
    "        \n",
    "        _conv1_w = model_graph.get_tensor_by_name('conv1/kernel:0')\n",
    "        _conv1_b = model_graph.get_tensor_by_name('conv1/bias:0')   \n",
    "\n",
    "        _conv2_w = model_graph.get_tensor_by_name('conv2/kernel:0')\n",
    "        _conv2_b = model_graph.get_tensor_by_name('conv2/bias:0')  \n",
    "\n",
    "        _conv4_w = model_graph.get_tensor_by_name('conv4/kernel:0')\n",
    "        _conv4_b = model_graph.get_tensor_by_name('conv4/bias:0')\n",
    "\n",
    "        _conv5_w = model_graph.get_tensor_by_name('conv5/kernel:0')\n",
    "        _conv5_b = model_graph.get_tensor_by_name('conv5/bias:0')\n",
    "        \n",
    "        _conv7_w = model_graph.get_tensor_by_name('conv7/kernel:0')\n",
    "        _conv7_b = model_graph.get_tensor_by_name('conv7/bias:0')\n",
    "        \n",
    "        _conv8_w = model_graph.get_tensor_by_name('conv8/kernel:0')\n",
    "        _conv8_b = model_graph.get_tensor_by_name('conv8/bias:0')\n",
    "    \n",
    "        _dict = {\n",
    "            'conv1_w': _conv1_w.eval(),\n",
    "            'conv1_b': _conv1_b.eval(),\n",
    "            'conv2_w': _conv2_w.eval(),\n",
    "            'conv2_b': _conv2_b.eval(),\n",
    "            'conv4_w': _conv4_w.eval(),\n",
    "            'conv4_b': _conv4_b.eval(),\n",
    "            'conv5_w': _conv5_w.eval(),\n",
    "            'conv5_b': _conv5_b.eval(),\n",
    "            'conv7_w': _conv7_w.eval(),\n",
    "            'conv7_b': _conv7_b.eval(),\n",
    "            'conv8_w': _conv8_w.eval(),\n",
    "            'conv8_b': _conv8_b.eval(),\n",
    "        }\n",
    "\n",
    "    return _dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3d(_input, params_w, params_b, stop_gradient=False, layer=None):\n",
    "    conv_w = tf.Variable(initial_value=params_w, name='w')\n",
    "    conv = tf.nn.conv3d(_input, conv_w, strides=[1, 1, 1, 1, 1], padding=\"SAME\")\n",
    "    conv_b = tf.Variable(initial_value=params_b, name='b')\n",
    "    biaed_conv = tf.nn.bias_add(conv, conv_b, name=\"conv\" + str(layer))\n",
    "    \n",
    "    if stop_gradient:\n",
    "        biaed_conv = tf.stop_gradient(biaed_conv, 'stop_gradient' + str(layer))\n",
    "    \n",
    "    return tf.nn.relu(biaed_conv, name=\"relu\" + str(layer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn3d_model_with_default(params, x_input, output_label_size, stop_layer=0, keep_rate=0.5, seed=None):\n",
    "    \n",
    "    assert(stop_layer<=10)\n",
    "    assert(stop_layer>=0)\n",
    "    \n",
    "    if seed is not None:\n",
    "        tf.set_random_seed(seed)\n",
    "    \n",
    "    stop_gradients = np.zeros(10)\n",
    "    for i in range(len(stop_gradients)):\n",
    "        if stop_layer >= i:\n",
    "            stop_gradients[i] = True\n",
    "        else:\n",
    "            stop_gradients[i] = False\n",
    "    \n",
    "    with tf.name_scope('conv1_layer'):\n",
    "        conv1 = conv3d(x_input, params['conv1_w'], params['conv1_b'], stop_gradient=stop_gradients[1], layer=1)\n",
    "\n",
    "    with tf.name_scope('conv2_layer'):\n",
    "        conv2 = conv3d(conv1, params['conv2_w'], params['conv2_b'], stop_gradient=stop_gradients[2], layer=2)\n",
    "\n",
    "    with tf.name_scope('pool3_layer'):\n",
    "        pool3 = tf.layers.max_pooling3d(inputs=conv2, pool_size=[2, 2, 2], strides=2, name=\"pool3\")\n",
    "\n",
    "    with tf.name_scope('conv4_layer'):\n",
    "        conv4 = conv3d(pool3, params['conv4_w'], params['conv4_b'], stop_gradient=stop_gradients[4], layer=4)\n",
    "\n",
    "    with tf.name_scope('conv5_layer'):\n",
    "        conv5 = conv3d(conv4, params['conv5_w'], params['conv5_b'], stop_gradient=stop_gradients[5], layer=5)\n",
    "        \n",
    "    with tf.name_scope('pool6_layer'):\n",
    "        pool6 = tf.layers.max_pooling3d(inputs=conv5, pool_size=[2, 2, 2], strides=2, name=\"pool6\")\n",
    "        \n",
    "    with tf.name_scope('conv7_layer'):\n",
    "        conv7 = conv3d(pool6, params['conv7_w'], params['conv7_b'], stop_gradient=stop_gradients[7], layer=7)\n",
    "        \n",
    "    with tf.name_scope('conv8_layer'):\n",
    "        conv8 = conv3d(conv7, params['conv8_w'], params['conv8_b'], stop_gradient=stop_gradients[8], layer=8)\n",
    "        \n",
    "    with tf.name_scope('pool9_layer'):\n",
    "        pool9 = tf.layers.max_pooling3d(inputs=conv8, pool_size=[2, 2, 2], strides=2, name=\"pool9\")\n",
    "           \n",
    "    with tf.name_scope(\"batch_norm\"):\n",
    "        cnn3d_bn = tf.layers.batch_normalization(inputs=pool9, training=False, name=\"bn\")\n",
    "        \n",
    "    with tf.name_scope(\"fully_con\"):\n",
    "        flattening = tf.reshape(cnn3d_bn, [-1, 4*4*4*512])\n",
    "        dense = tf.layers.dense(inputs=flattening, units=1024, activation=tf.nn.relu, name=\"full_con\")\n",
    "        # (1-keep_rate) is the probability that the node will be kept\n",
    "        dropout = tf.layers.dropout(inputs=dense, rate=keep_rate, training=False, name=\"dropout\")\n",
    "        \n",
    "    with tf.name_scope(\"y_conv\"):\n",
    "        y_conv = tf.layers.dense(inputs=dropout, units=output_label_size, name=\"y_pred\") \n",
    "        \n",
    "    return y_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# one hot indexes\n",
    "\n",
    "import os\n",
    "model_path = os.path.join(os.getcwd(), 'trained_model', 'model-0')\n",
    "\n",
    "params = get_model_params(model_path)\n",
    "\n",
    "learning_rate = 40000000\n",
    "\n",
    "file_num =1000\n",
    "\n",
    "x_guitar = get_data(data_path + \"/guitar\", max_file_num=file_num) \n",
    "x_table = get_data(data_path + \"/table\", max_file_num=file_num)\n",
    "x_ = x_guitar + x_table\n",
    "y_ = np.zeros((len(x_),6))\n",
    "\n",
    "for index, _ in enumerate(y_):\n",
    "    if index < len(x_guitar):\n",
    "        y_[index][5] = 1\n",
    "    else:\n",
    "        y_[index][3] = 1\n",
    "        \n",
    "split_point = int(0.7*len(x_))\n",
    "x_train = x_[:split_point]\n",
    "y_train = y_[:split_point]\n",
    "\n",
    "x_test = x_[split_point:]\n",
    "y_test = y_[split_point:]\n",
    "    \n",
    "\n",
    "device_name = '/gpu:1' \n",
    "\n",
    "with tf.Session(graph=tf.Graph()) as sess:\n",
    "    with tf.device(device_name):\n",
    "        with tf.name_scope('inputs'):\n",
    "            x_input = tf.placeholder(tf.float32, shape=[None, 32, 32, 32, 1], name=\"x_input\")\n",
    "            y_input = tf.placeholder(tf.float32, shape=[None, 6], name=\"y_input\") \n",
    "\n",
    "        prediction = cnn3d_model_with_default(params, x_input, 6, stop_layer=10, seed=1234)\n",
    "        tf.add_to_collection(\"logits\", prediction)\n",
    "\n",
    "        with tf.name_scope(\"cross_entropy\"):\n",
    "            cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=y_input), name=\"cross_entropy\")\n",
    "\n",
    "        with tf.name_scope(\"training\"):\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "            tf.add_to_collection(\"optimizer\", optimizer)\n",
    "\n",
    "        correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y_input, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, 'float'), name=\"acc\")\n",
    "    \n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    graph = tf.get_default_graph()\n",
    "    \n",
    "    _conv1_w_prev = graph.get_tensor_by_name('conv1_layer/w:0').eval()\n",
    "    _conv1_b_prev = graph.get_tensor_by_name('conv1_layer/b:0').eval()\n",
    "    \n",
    "    _conv7_w_prev = graph.get_tensor_by_name('conv8_layer/w:0').eval()\n",
    "    _conv7_b_prev = graph.get_tensor_by_name('conv8_layer/b:0').eval()\n",
    "    \n",
    "    batch_size = 32\n",
    "    iterations_train = int(len(x_train)/batch_size) +1\n",
    "    iterations_test= int(len(x_test)/batch_size) +1\n",
    "    \n",
    "    acc = 0\n",
    "    \n",
    "    for itr in range(iterations_test):\n",
    "        mini_batch_x = x_test[itr*batch_size: (itr+1)*batch_size]\n",
    "        mini_batch_y = y_test[itr*batch_size: (itr+1)*batch_size]\n",
    "        _acc = sess.run(accuracy, feed_dict={x_input: mini_batch_x, y_input: mini_batch_y})\n",
    "        acc += _acc\n",
    "        print('\\tLost for', itr+1, \"/\", iterations_test, _acc, end='\\r')\n",
    "    \n",
    "    print('\\n')\n",
    "    print('Test Acc:', acc/iterations_test)\n",
    "    \n",
    "    for itr in range(iterations_train):\n",
    "        mini_batch_x = x_train[itr*batch_size: (itr+1)*batch_size]\n",
    "        mini_batch_y = y_train[itr*batch_size: (itr+1)*batch_size]\n",
    "        _optimizer, _cost = sess.run([optimizer, cost], feed_dict={x_input: mini_batch_x, y_input: mini_batch_y})\n",
    "        print('\\tLost for', itr+1, \"/\", iterations_train, _cost, end='\\r')\n",
    "    \n",
    "    print('\\n')\n",
    "        \n",
    "    _conv1_w_aft = graph.get_tensor_by_name('conv1_layer/w:0').eval()\n",
    "    _conv1_b_aft = graph.get_tensor_by_name('conv1_layer/b:0').eval()\n",
    "    \n",
    "    _conv7_w_aft = graph.get_tensor_by_name('conv8_layer/w:0').eval()\n",
    "    _conv7_b_aft = graph.get_tensor_by_name('conv8_layer/b:0').eval()\n",
    "    \n",
    "    print((_conv1_w_prev == _conv1_w_aft).all())\n",
    "    print((_conv1_b_prev == _conv1_b_aft).all())\n",
    "    \n",
    "    print((_conv7_w_prev == _conv7_w_aft).all())\n",
    "    print((_conv7_b_prev == _conv7_b_aft).all())\n",
    "    \n",
    "    acc = 0\n",
    "    for itr in range(iterations_test):\n",
    "        mini_batch_x = x_test[itr*batch_size: (itr+1)*batch_size]\n",
    "        mini_batch_y = y_test[itr*batch_size: (itr+1)*batch_size]\n",
    "        _acc = sess.run(accuracy, feed_dict={x_input: mini_batch_x, y_input: mini_batch_y})\n",
    "        acc += _acc\n",
    "        print('\\tLost for', itr+1, \"/\", iterations_test, cost, end='\\r')\n",
    "    \n",
    "    print('\\n')\n",
    "    print('Test Acc:', acc/iterations_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "File C:\\Users\\212606295\\Desktop\\3D-CNN\\3d_pointcloud\\trained_model\\model-0.meta does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-65-317074141b34>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'trained_model'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'model-0'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_model_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mfile_num\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;36m7000\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-a1cd4767ea9a>\u001b[0m in \u001b[0;36mget_model_params\u001b[1;34m(model_path)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;31m# Restore model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0msaver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimport_meta_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_path\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\".meta\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[0msaver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36mimport_meta_graph\u001b[1;34m(meta_graph_or_file, clear_devices, import_scope, **kwargs)\u001b[0m\n\u001b[0;32m   1801\u001b[0m                      \"execution is enabled.\")\n\u001b[0;32m   1802\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmeta_graph_or_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeta_graph_pb2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMetaGraphDef\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1803\u001b[1;33m     \u001b[0mmeta_graph_def\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmeta_graph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_meta_graph_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmeta_graph_or_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1804\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1805\u001b[0m     \u001b[0mmeta_graph_def\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmeta_graph_or_file\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\meta_graph.py\u001b[0m in \u001b[0;36mread_meta_graph_file\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m    553\u001b[0m   \u001b[0mmeta_graph_def\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmeta_graph_pb2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMetaGraphDef\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mfile_io\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfile_exists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 555\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"File %s does not exist.\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    556\u001b[0m   \u001b[1;31m# First try to read it as a binary file.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m   \u001b[0mfile_content\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfile_io\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFileIO\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: File C:\\Users\\212606295\\Desktop\\3D-CNN\\3d_pointcloud\\trained_model\\model-0.meta does not exist."
     ]
    }
   ],
   "source": [
    "# one hot indexes\n",
    "# 2: chair , 3: car\n",
    "\n",
    "import os\n",
    "model_path = os.path.join(os.getcwd(), 'trained_model', 'model-0')\n",
    "\n",
    "params = get_model_params(model_path)\n",
    "\n",
    "file_num =7000\n",
    "\n",
    "a = get_data(data_path + \"/airplane\", max_file_num=file_num)\n",
    "    \n",
    "device_name = '/gpu:1' \n",
    "\n",
    "with tf.Session(graph=tf.Graph()) as sess:\n",
    "    with tf.device(device_name):\n",
    "        with tf.name_scope('inputs'):\n",
    "            x_input = tf.placeholder(tf.float32, shape=[None, 32, 32, 32, 1], name=\"x_input\")\n",
    "            y_input = tf.placeholder(tf.float32, shape=[None, 5], name=\"y_input\") \n",
    "\n",
    "        prediction = cnn3d_model_with_default(params, x_input, 5, stop_layer=10, seed=1234)\n",
    "        \n",
    "        correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y_input, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, 'float'), name=\"acc\")\n",
    "    \n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    graph = tf.get_default_graph()\n",
    "    \n",
    "    batch_size = 32\n",
    "    accu = []\n",
    "    iterations_test = int(file_num/batch_size) +1\n",
    "    for itr in range(iterations_test):\n",
    "        mini_batch_x = a[itr*batch_size: (itr+1)*batch_size]\n",
    "        mini_batch_y = y[itr*batch_size: (itr+1)*batch_size]\n",
    "#         acc = sess.run(accuracy, feed_dict={x_input: mini_batch_x, y_input: mini_batch_y})\n",
    "        if len(mini_batch_x) == 0:\n",
    "            continue\n",
    "        p = np.argmax(sess.run(prediction, feed_dict={x_input: mini_batch_x}), 1)\n",
    "        accu.append(p)\n",
    "        print('\\taccuracy for', itr+1, \"/\", iterations_train, p, end='\\r')\n",
    "    \n",
    "    print('\\n')\n",
    "    \n",
    "    pppp = {}\n",
    "    for _ in range(len(accu)):\n",
    "        for i,v in enumerate(accu[_]):\n",
    "            if accu[_][i] in pppp:\n",
    "                pppp[v] = pppp[v] + 1\n",
    "            else:\n",
    "                pppp[v] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 1343, 1: 476, 2: 139, 3: 2684, 4: 2346, 5: 12}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
